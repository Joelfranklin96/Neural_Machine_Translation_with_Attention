{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "### v1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14508,
     "status": "ok",
     "timestamp": 1612468511651,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "RcBRMzPiiMmp",
    "outputId": "17e9a429-5bb6-4401-a23a-f8f756d6113d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16981,
     "status": "ok",
     "timestamp": 1612468514155,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "gwIf5l17h3Mg",
    "outputId": "1fca5fb8-3a9b-4a78-f726-7aef8e14ee41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 24544.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# The model built here can be used to translate from one language to another. But Language Translation requires massive datasets\n",
    "# and takes days of training on GPUs.\n",
    "# So here we will perform a simpler 'date translation' task where the\n",
    "# 'the 29th of August 1958' will be translated to '1958-08-29'\n",
    "# '03/30/1968' will be translated to '1968-03-30'\n",
    "# '24 JUNE 1987' will be translated to '1987-06-24'\n",
    "# human readable date will be translated to machine readable date\n",
    "\n",
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n",
    "\n",
    "# dataset is a list of tuples and each tuple is a pair of (input, output)\n",
    "# 'human_vocab' is a dictionary mapping each character in human_readable_date(input) to an integer-valued index\n",
    "# 'machine_vocab' is a dictionary mapping each character in machine_readable_date(output) to an integer-valued index\n",
    "\n",
    "# There are 37 different characters in the 'human_vocab'. len(human_vocab) = 37\n",
    "# There are 11 different characters in the 'machine_vocab'. len(machine_vocab) = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16972,
     "status": "ok",
     "timestamp": 1612468514156,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "zCTqMyPch3Mg",
    "outputId": "42c9d8aa-d07b-4618-ab8a-4db4e1b971e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('27 november 1980', '1980-11-27'),\n",
       " ('friday september 13 2019', '2019-09-13'),\n",
       " ('tuesday july 17 2018', '2018-07-17'),\n",
       " ('4/10/19', '2019-04-10'),\n",
       " ('wednesday april 27 1977', '1977-04-27'),\n",
       " ('tuesday december 6 1977', '1977-12-06'),\n",
       " ('01 sep 1991', '1991-09-01'),\n",
       " ('3 10 22', '2022-10-03'),\n",
       " ('tuesday july 20 1999', '1999-07-20'),\n",
       " ('wednesday january 29 1992', '1992-01-29')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16962,
     "status": "ok",
     "timestamp": 1612468514157,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "Qdso90EBh3Mg",
    "outputId": "0a364ad8-8b25-4de3-f036-d5d8e40bdf8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)\n",
    "\n",
    "# We set Tx = 30 which is the maximum length of the human readable date in our dataset\n",
    "# We set Ty = 10 as the machine_readable_date is exactly 10 characters long\n",
    "\n",
    "# The following preprocessing is done\n",
    "\n",
    "# X: a processed version of the human readable dates in the training set.\n",
    "#    - Each character in X is replaced by an index (integer) mapped to the character using human_vocab.\n",
    "#    - Each date is padded to ensure a length of  ð‘‡ð‘¥ using a special character (< pad >).\n",
    "#    - X.shape = (m, Tx) where m is the number of training examples in a batch.\n",
    "\n",
    "# Y: a processed version of the machine readable dates in the training set.\n",
    "#    - Each character is replaced by an index (integer) mapped to the character using machine_vocab.\n",
    "#    - Y.shape = (m, Ty)\n",
    "\n",
    "# Xoh: one-hot version of X\n",
    "\n",
    "# Yoh: one-hot version of Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16952,
     "status": "ok",
     "timestamp": 1612468514158,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "kUOayR4gh3Mh",
    "outputId": "d20994de-bbea-4cc7-ffaf-38a05974c9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 27 november 1980\n",
      "Target date: 1980-11-27\n",
      "\n",
      "Source after preprocessing (indices): [ 5 10  0 25 26 32 17 24 14 17 28  0  4 12 11  3 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10  9  1  0  2  2  0  3  8]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Let's look at an example of the preprocessed training example\n",
    "\n",
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 16950,
     "status": "ok",
     "timestamp": 1612468514158,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "Cvop5Apyh3Mm"
   },
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "# A vector of shape (m,n_s) becomes (m,Tx,n_s) after repeating\n",
    "repeator = RepeatVector(Tx)\n",
    "\n",
    "\n",
    "concatenator = Concatenate(axis=-1)\n",
    "# concatenator(a,b) where a is of dimension (x,y,z1) and b is of dimension (x,y,z2). \n",
    "# All the dimensions need to match except the last dimension\n",
    "# The resultant is of dimension (x,y,z1+z2)\n",
    "\n",
    "# The 'Dense' layer only transforms the last dimension.\n",
    "# When you pass a tensor of size (x,y,z) through the 'densor1', the resultant is of size (x,y,10).\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "# When you pass a tensor through a Dense layer, it multiplies the values in the last dimension of the input tensor with \n",
    "# the layer's weights and adds the bias, followed by applying the activation function, resulting in a new last dimension as \n",
    "# specified by the number of units in the Dense layer.\n",
    "\n",
    "activator = Activation(softmax, name='attention_weights') \n",
    "# We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "\n",
    "dotor = Dot(axes = 1)\n",
    "\n",
    "\n",
    "# Note\n",
    "\n",
    "# Question - The advantage of attention model is that it doesn't have to wait till processing the entire sentence and can \n",
    "# start the language translation even after few words of the input sentence are processed right? but don't we take \n",
    "# attention on x<1>, x<2>,....x<tx> all into consideration? Aren't we still processing the complete input sentence and \n",
    "# then only y<1> is being produced right?\n",
    "\n",
    "# Answer\n",
    "# Models that don't use attention put equal emphasis on every part of the input sentence whereas models that \n",
    "# use attention puts more emphasis on relevant parts of the input sentence. This is the only difference. \n",
    "# Otherwise both models with and without attention does depend on the entire input sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 16950,
     "status": "ok",
     "timestamp": 1612468514159,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "mZuMOnTDh3Mn"
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attention) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # s_prev is of shape (m, n_s)\n",
    "    # Use 'repeator' to repeat s_prev to be of shape (m, Tx, n_s) so that we can concatenate it with all hidden states \"a\"\n",
    "    s_prev = repeator(s_prev)\n",
    "    \n",
    "    # Use 'concatenator' to concatenate a and s_prev on the last axis\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # 'concat' is of shape (m, Tx, 2*n_a + n_s)\n",
    "    \n",
    "    # Use 'densor1' to propagate concat through a small fully-connected neural network to compute the \n",
    "    # \"intermediate energies\" variable e. \n",
    "    e = densor1(concat)\n",
    "    # 'e' is of shape (m, Tx, 10)\n",
    "    \n",
    "    # When you pass a tensor through a Dense layer, it multiplies the values in the last dimension of the input tensor with \n",
    "    # the layer's weights and adds the bias, followed by applying the activation function, resulting in a new last dimension as \n",
    "    # specified by the number of units in the Dense layer.\n",
    "    \n",
    "    # Use 'densor2' to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies.\n",
    "    energies = densor2(e)\n",
    "    # 'energies' is of shape (m, Tx, 1)\n",
    "    \n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\"\n",
    "    alphas = activator(energies)\n",
    "    # 'alphas' is of shape (m,Tx,1)\n",
    "    # 'alphas' matrix values change when context<t> changes (as timestep t changes).\n",
    "    \n",
    "    # Use dotor together with \"alphas\" and \"a\", to compute the context vector to be given to the next (post-attention) LSTM-cell\n",
    "    context = dotor([alphas,a])\n",
    "    # For every 2*n_a vector in the 'a', there is one value in 'alphas' which is along the T_x direction. \n",
    "    # This value will be multiplied throughout all values of the 2*n_a vector. Basically we are scaling the 2*n_a vector.\n",
    "    # The scaling process is done for each time step and the vectors along the T_x direction are added.\n",
    "    # The shape of 'context' is (m, 2*n_a)\n",
    "    \n",
    "    # Another alternative method that we could have done\n",
    "    # context = np.sum(alphas*a, axis = 1)\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "def one_step_attention_test(target):\n",
    "\n",
    "    m = 10\n",
    "    Tx = 30\n",
    "    n_a = 32\n",
    "    n_s = 64\n",
    "    #np.random.seed(10)\n",
    "    a = np.random.uniform(1, 0, (m, Tx, 2 * n_a)).astype(np.float32)\n",
    "    s_prev =np.random.uniform(1, 0, (m, n_s)).astype(np.float32) * 1\n",
    "    context = target(a, s_prev)\n",
    "    \n",
    "    assert type(context) == tf.python.framework.ops.EagerTensor, \"Unexpected type. It should be a Tensor\"\n",
    "    assert tuple(context.shape) == (m, 1, n_s), \"Unexpected output shape\"\n",
    "    assert np.all(context.numpy() > 0), \"All output values must be > 0 in this example\"\n",
    "    assert np.all(context.numpy() < 1), \"All output values must be < 1 in this example\"\n",
    "\n",
    "    #assert np.allclose(context[0][0][0:5].numpy(), [0.50877404, 0.57160693, 0.45448175, 0.50074816, 0.53651875]), \"Unexpected values in the result\"\n",
    "    print(\"\\033[92mAll tests passed!\")\n",
    "    \n",
    "one_step_attention_test(one_step_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 16949,
     "status": "ok",
     "timestamp": 1612468514159,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "5RHgmZrVh3Mo"
   },
   "outputs": [],
   "source": [
    "n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "# This is the post attention LSTM cell.  \n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # Please do not modify this global variable.\n",
    "# In a standard LSTM, return_state=True will return the last hidden state (a<Tx>) and the last cell state (c<Tx>), along with \n",
    "# the output sequence (if return_sequences=True) ([a<1>,a<2>,....a<Tx>]) or the last output (a<Tx>) (if return_sequences=False).\n",
    "\n",
    "# In a standard LSTM, if return_state is False, then it will not return the last cell state (c<Tx>) and will only return \n",
    "# the last hidden state a<Tx>.\n",
    "\n",
    "# In a Bidirectional LSTM, return_sequences=True ensures that you get the output (hidden states) from both directions for \n",
    "# each time step of the input sequence.\n",
    "\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 16948,
     "status": "ok",
     "timestamp": 1612468514160,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "qeKbeDOvh3Mo"
   },
   "outputs": [],
   "source": [
    "def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx, human_vocab_size)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    \n",
    "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
    "    # for the decoder LSTM with shape (n_s,)\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    # Note the 'comma' is required in 'shape = (n_s,)' because in Keras when we define an input shape, we have to pass a tuple \n",
    "    # representing the dimensions of the input.\n",
    "    # 's0' and 'c0' are just for 1 timestep (the initial one)\n",
    "    \n",
    "    # hidden state\n",
    "    s = s0\n",
    "    # cell state\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Define the pre-attention Bi-LSTM\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences = True), merge_mode='concat')(X)\n",
    "    # In a Bidirectional LSTM, return_sequences=True ensures that you get the output (hidden states) from both directions for \n",
    "    # each time step of the input sequence.\n",
    "    # LSTM, Keras automatically initializes the hidden state and the cell state to zero vectors by default if we don't\n",
    "    # specify them. And automatically c<1>,a<1> are used to calculate a<2>.\n",
    "    \n",
    "    # Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Perform one step of the attention mechanism to get back the context vector at step t\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        _, s, c = post_activation_LSTM_cell(context, initial_state = [s,c])\n",
    "        \n",
    "        # Note - The below line of code doesn't work because when we do it iteratively in keras we have to explicitly mention\n",
    "        # the inputs (_, s, c = post_activation_LSTM_cell(initial_state = [s,c])(context))\n",
    "        \n",
    "        # Apply Dense layer to the hidden state output of the post-attention LSTM\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Append \"out\" to the \"outputs\" list\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Create model instance taking three inputs and returning the list of outputs\n",
    "    model = Model(inputs=[X,s0,c0], outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Note 1\n",
    "# The pre-attention Bidirectional LSTM is called the 'encoder'. Usually the 'encoder' calculates the output all at once. This is\n",
    "# because the output of one time step is not fed as input to calculate the output of next time step.\n",
    "# Whereas the 'post_activation_LSTM_cell' is called the 'decoder'. The 'context' of each time step is fed as input to calculate\n",
    "# the output of next time step. Hence we calculate the outputs of the time steps iteratively.\n",
    "\n",
    "# Note 2\n",
    "# We could have defined 's0' and 'c0' to be zero vectors of respective sizes (n_s) and \n",
    "# could have defined 'model = Model(inputs=[X], outputs=outputs)'.\n",
    "# But if 's0' and 'c0' are hardcoded, it would decrease the flexibility and they would always be zero vectors and\n",
    "# can't be input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['InputLayer', [(None, 30, 37)], 0], ['InputLayer', [(None, 64)], 0], ['Bidirectional', (None, 30, 64), 17920], ['RepeatVector', (None, 30, 64), 0, 30], ['Concatenate', (None, 30, 128), 0], ['Dense', (None, 30, 10), 1290, 'tanh'], ['Dense', (None, 30, 1), 11, 'relu'], ['Activation', (None, 30, 1), 0], ['Dot', (None, 1, 64), 0], ['InputLayer', [(None, 64)], 0], ['LSTM', [(None, 64), (None, 64), (None, 64)], 33024, [(None, 1, 64), (None, 64), (None, 64)], 'tanh'], ['Dense', (None, 11), 715, 'softmax']]\n",
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "from test_utils import *\n",
    "\n",
    "def modelf_test(target):\n",
    "    Tx = 30\n",
    "    n_a = 32\n",
    "    n_s = 64\n",
    "    len_human_vocab = 37\n",
    "    len_machine_vocab = 11\n",
    "    \n",
    "    \n",
    "    model = target(Tx, Ty, n_a, n_s, len_human_vocab, len_machine_vocab)\n",
    "    \n",
    "    print(summary(model))\n",
    "\n",
    "    \n",
    "    expected_summary = [['InputLayer', [(None, 30, 37)], 0],\n",
    "                         ['InputLayer', [(None, 64)], 0],\n",
    "                         ['Bidirectional', (None, 30, 64), 17920],\n",
    "                         ['RepeatVector', (None, 30, 64), 0, 30],\n",
    "                         ['Concatenate', (None, 30, 128), 0],\n",
    "                         ['Dense', (None, 30, 10), 1290, 'tanh'],\n",
    "                         ['Dense', (None, 30, 1), 11, 'relu'],\n",
    "                         ['Activation', (None, 30, 1), 0],\n",
    "                         ['Dot', (None, 1, 64), 0],\n",
    "                         ['InputLayer', [(None, 64)], 0],\n",
    "                         ['LSTM',[(None, 64), (None, 64), (None, 64)], 33024,[(None, 1, 64), (None, 64), (None, 64)],'tanh'],\n",
    "                         ['Dense', (None, 11), 715, 'softmax']]\n",
    "\n",
    "    assert len(model.outputs) == 10, f\"Wrong output shape. Expected 10 != {len(model.outputs)}\"\n",
    "\n",
    "    comparator(summary(model), expected_summary)\n",
    "    \n",
    "\n",
    "modelf_test(modelf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 20837,
     "status": "ok",
     "timestamp": 1612468518050,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "psdd-Ac6h3Mp"
   },
   "outputs": [],
   "source": [
    "model = modelf(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20835,
     "status": "ok",
     "timestamp": 1612468518050,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "tX0vaYmPh3Mq",
    "outputId": "336b9248-70b0-4379-be95-95366874c02a"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 20835,
     "status": "ok",
     "timestamp": 1612468518051,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "sBFRJ49rh3Ms"
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01) # Adam(...) \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# UNIT TESTS\n",
    "assert opt.lr == 0.005, \"Set the lr parameter to 0.005\"\n",
    "assert opt.beta_1 == 0.9, \"Set the beta_1 parameter to 0.9\"\n",
    "assert opt.beta_2 == 0.999, \"Set the beta_2 parameter to 0.999\"\n",
    "assert opt.decay == 0.01, \"Set the decay parameter to 0.01\"\n",
    "assert model.loss == \"categorical_crossentropy\", \"Wrong loss. Use 'categorical_crossentropy'\"\n",
    "assert model.optimizer == opt, \"Use the optimizer that you have instantiated\"\n",
    "assert model.compiled_metrics._user_metrics[0] == 'accuracy', \"set metrics to ['accuracy']\"\n",
    "\n",
    "print(\"\\033[92mAll tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 20833,
     "status": "ok",
     "timestamp": 1612468518051,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "USFiNKYhh3Mt"
   },
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "\n",
    "# The 'outputs' to be fed to the model is a list of Ty elements where each element is of shape (m,len(machine_vocab)). \n",
    "# But 'Yoh' is of shape (m,Ty,len(machine_vocab))\n",
    "# So we swap the axes 0 and 1 to make 'Yoh' of the shape (Ty,m,len(machine_vocab)).\n",
    "# Then we do 'list(swapped_Yoh)' so that 'Yoh' becomes a list consisting of Ty elements where \n",
    "# each element of size (m,Ty,len(machine_vocab))\n",
    "\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47944,
     "status": "ok",
     "timestamp": 1612468545172,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "tPuwY45bh3Mt",
    "outputId": "ec9dfc4c-1dcb-4577-d872-474f79c60d5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 9s 86ms/step - loss: 16.5680 - dense_5_loss: 1.1589 - dense_5_1_loss: 1.0174 - dense_5_2_loss: 1.8156 - dense_5_3_loss: 2.6593 - dense_5_4_loss: 0.7361 - dense_5_5_loss: 1.2673 - dense_5_6_loss: 2.6730 - dense_5_7_loss: 0.9684 - dense_5_8_loss: 1.7123 - dense_5_9_loss: 2.5598 - dense_5_accuracy: 0.5120 - dense_5_1_accuracy: 0.6828 - dense_5_2_accuracy: 0.2907 - dense_5_3_accuracy: 0.0836 - dense_5_4_accuracy: 0.9513 - dense_5_5_accuracy: 0.3205 - dense_5_6_accuracy: 0.0500 - dense_5_7_accuracy: 0.9391 - dense_5_8_accuracy: 0.2542 - dense_5_9_accuracy: 0.1061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa59e17cb50>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 47942,
     "status": "ok",
     "timestamp": 1612468545173,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "ooiZCOx0h3Mu"
   },
   "outputs": [],
   "source": [
    "# The below model has been run for longer time and the weights have been saved\n",
    "\n",
    "model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53835,
     "status": "ok",
     "timestamp": 1612468551077,
     "user": {
      "displayName": "Mubsi K",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gip7OjOkdNkKxKDyWEQAq1o8ccGN_HrBTGdqjgQ=s64",
      "userId": "08094225471505108399"
     },
     "user_tz": -300
    },
    "id": "rQ8sd_cuh3Mv",
    "outputId": "c37e92ac-5c60-4caf-b843-6aaeaa37be25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 3 May 1979\n",
      "output: 1979-05-33 \n",
      "\n",
      "source: 5 April 09\n",
      "output: 2009-04-05 \n",
      "\n",
      "source: 21th of August 2016\n",
      "output: 2016-08-20 \n",
      "\n",
      "source: Tue 10 Jul 2007\n",
      "output: 2007-07-10 \n",
      "\n",
      "source: Saturday May 9 2018\n",
      "output: 2018-05-09 \n",
      "\n",
      "source: March 3 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: March 3rd 2001\n",
      "output: 2001-03-03 \n",
      "\n",
      "source: 1 March 2001\n",
      "output: 2001-03-01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Results with the saved model weights\n",
    "\n",
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "s00 = np.zeros((1, n_s))\n",
    "c00 = np.zeros((1, n_s))\n",
    "for example in EXAMPLES:\n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    #print(source)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    source = np.swapaxes(source, 0, 1)\n",
    "    source = np.expand_dims(source, axis=0)\n",
    "    prediction = model.predict([source, s00, c00])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note\n",
    "# Machine translation models can be used to map from one sequence to another. \n",
    "# They are useful not just for translating human languages (like French->English) but also for tasks like \n",
    "# date format translation.\n",
    "\n",
    "# A network using an attention mechanism can translate from inputs of length Tx to outputs of length Ty, where\n",
    "# Tx and Ty can be different.\n",
    "\n",
    "# The model that we built here can be used to translate from one language to another, such as translating from English to Hindi.\n",
    "# However, language translation requires massive datasets and usually takes days of training on GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
